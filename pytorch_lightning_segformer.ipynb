{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cplus/projects/m.tarek_master/m.tarek/m.tarek/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2024-02-20 16:42:08.872778: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-20 16:42:08.894190: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-20 16:42:09.266271: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor ,SegformerFeatureExtractor\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as aug\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import evaluate\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSegmentationDataset(Dataset):\n",
    "    \"\"\"Image segmentation dataset.\"\"\"\n",
    "    def __init__(self, root_dir, feature_extractor, transforms=None, train=True):\n",
    "        super(ImageSegmentationDataset,self).__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.train = train\n",
    "        self.transforms = transforms\n",
    "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, \"pngmasks\")\n",
    "        image_file_names = []\n",
    "        for root, dirs, files in os.walk(self.img_dir):\n",
    "            image_file_names.extend(files)\n",
    "        self.images = sorted(image_file_names)\n",
    "        # read annotations\n",
    "        annotation_file_names = []\n",
    "        for root, dirs, files in os.walk(self.ann_dir):\n",
    "            annotation_file_names.extend(files)\n",
    "        self.annotations = sorted(annotation_file_names)\n",
    "        assert len(self.images) == len(self.annotations) or len(self.images)==0 ,  \"There must be as many images as there are segmentation maps\"\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(os.path.join(self.img_dir, self.images[idx]))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        segmentation_map = cv2.imread(os.path.join(self.ann_dir, self.annotations[idx]))\n",
    "        segmentation_map = cv2.cvtColor(segmentation_map, cv2.COLOR_BGR2GRAY)\n",
    "        if self.transforms is not None:\n",
    "            augmented = self.transforms(image=image, mask=segmentation_map)\n",
    "            encoded_inputs = self.feature_extractor(augmented['image'], augmented['mask'], return_tensors=\"pt\")\n",
    "        else:\n",
    "            encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors=\"pt\")\n",
    "        for k,v in encoded_inputs.items():\n",
    "            encoded_inputs[k].squeeze_()\n",
    "\n",
    "        return encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = aug.Compose([\n",
    "    aug.Flip(p=0.5),\n",
    "],is_check_shapes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir =r\"/home/cplus/projects/m.tarek_master/m.tarek/graval detection project/datasets/under_water_masks_dataset/train\"\n",
    "valid_dir=r\"/home/cplus/projects/m.tarek_master/m.tarek/graval detection project/datasets/under_water_masks_dataset/val\"\n",
    "test_dir=r\"/home/cplus/projects/m.tarek_master/m.tarek/graval detection project/datasets/under_water_masks_dataset/test\"\n",
    "feature_extractor = SegformerImageProcessor.from_pretrained (\"nvidia/mit-b5\")\n",
    "train_dataset = ImageSegmentationDataset(root_dir=train_dir, feature_extractor=feature_extractor, transforms=transform)\n",
    "valid_dataset = ImageSegmentationDataset(root_dir=valid_dir, feature_extractor=feature_extractor, transforms=None, train=False)\n",
    "test_dataset = ImageSegmentationDataset(root_dir=test_dir, feature_extractor=feature_extractor, transforms=None, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"stone\"]\n",
    "print(classes)\n",
    "id2label = {0:classes[0]}\n",
    "print(id2label)\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1,shuffle=False,num_workers=0)\n",
    "test_dataloader  = DataLoader(test_dataset,batch_size=1,shuffle=False,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegformerFinetuner(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, id2label, train_dataloader=None, val_dataloader=None, test_dataloader=None, metrics_interval=100):\n",
    "        super(SegformerFinetuner, self).__init__()\n",
    "        self.id2label = id2label\n",
    "        self.metrics_interval = metrics_interval\n",
    "        self.train_dl = train_dataloader\n",
    "        self.val_dl = val_dataloader\n",
    "        self.test_dl = test_dataloader\n",
    "        self.num_classes = len(id2label.keys())\n",
    "        self.label2id = {v:k for k,v in self.id2label.items()}\n",
    "        self.model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b5\", ignore_mismatched_sizes=True,\n",
    "                                                         reshape_last_stage=True)\n",
    "        self.train_mean_iou = evaluate.load(\"mean_iou\")\n",
    "        self.val_mean_iou = evaluate.load(\"mean_iou\")\n",
    "        self.test_mean_iou = evaluate.load(\"mean_iou\")\n",
    "        \n",
    "    def forward(self, images, masks):\n",
    "        outputs = self.model(pixel_values=images, labels=masks)\n",
    "        return(outputs)\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        outputs = self(images, masks)\n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        upsampled_logits = nn.functional.interpolate(logits, size=masks.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "        self.train_mean_iou.add_batch(predictions=predicted.detach().cpu().numpy(), references=masks.detach().cpu().numpy())\n",
    "        if batch_nb % self.metrics_interval == 0:\n",
    "            metrics = self.train_mean_iou.compute(num_labels=self.num_classes, ignore_index=255, reduce_labels=False,)\n",
    "            metrics = {'loss': loss, \"mean_iou\": metrics[\"mean_iou\"], \"mean_accuracy\": metrics[\"mean_accuracy\"]}\n",
    "            for k,v in metrics.items():\n",
    "                self.log(k,v)\n",
    "            return(metrics)\n",
    "        else:\n",
    "            return({'loss': loss})\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        outputs = self(images, masks)\n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        upsampled_logits = nn.functional.interpolate(logits, size=masks.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "        self.val_mean_iou.add_batch(predictions=predicted.detach().cpu().numpy(), references=masks.detach().cpu().numpy())\n",
    "        val_metrics = self.val_mean_iou.compute(num_labels=self.num_classes, ignore_index=255, reduce_labels=False)\n",
    "        val_metrics = {'val_loss': loss, \"val_mean_iou\": val_metrics[\"mean_iou\"], \"val_mean_accuracy\": val_metrics[\"mean_accuracy\"]}\n",
    "        \n",
    "        for k,v in val_metrics.items():\n",
    "                self.log(k,v)\n",
    "        return(val_metrics)\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        outputs = self(images, masks)\n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        upsampled_logits = nn.functional.interpolate(logits, size=masks.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "        test_metircs=self.test_mean_iou.add_batch(predictions=predicted.detach().cpu().numpy(), references=masks.detach().cpu().numpy())\n",
    "        test_metircs = {'test_loss': loss, \"test_mean_iou\": test_metircs[\"mean_iou\"], \"test_mean_accuracy\":test_metircs[\"mean_accuracy\"]}\n",
    "        for k,v in test_metircs.items():\n",
    "                self.log(k,v)\n",
    "        return(test_metircs)\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=2e-05, eps=1e-08)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.train_dl\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.val_dl\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self.test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SegformerFineTuner=SegformerFinetuner(id2label=id2label,train_dataloader=train_dataloader,val_dataloader=valid_dataloader,test_dataloader=test_dataloader,metrics_interval=10)\n",
    "SegformerFineTuner.to(device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model=SegformerFineTuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    max_epochs=N_EPOCHS,\n",
    "    gpus=1,\n",
    "    progress_bar_refresh_rate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=pl.Trainer(max_epochs=10,val_check_interval=len(valid_dataloader),accelerator=\"gpu\",devices=1,callbacks=[early_stop_callback, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model=SegformerFineTuner,val_dataloaders=valid_dataloader,train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard \n",
    "%tensorboard --logdir lightning_logs/ --port 6063"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = trainer.test(ckpt_path=\"best\",test_dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(res[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = {\n",
    "    0:(0,0,0),\n",
    "    1:(255,0,0),\n",
    "}\n",
    "\n",
    "def prediction_to_vis(prediction):\n",
    "    vis_shape = prediction.shape + (3,)\n",
    "    vis = np.zeros(vis_shape)\n",
    "    for i,c in color_map.items():\n",
    "        vis[prediction == i] = color_map[i]\n",
    "    return Image.fromarray(vis.astype(np.uint8))\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    images, masks = batch['pixel_values'], batch['labels']\n",
    "    outputs = SegformerFineTuner.model(images, masks)\n",
    "        \n",
    "    loss, logits = outputs[0], outputs[1]\n",
    "\n",
    "    upsampled_logits = nn.functional.interpolate(\n",
    "        logits, \n",
    "        size=masks.shape[-2:], \n",
    "        mode=\"bilinear\", \n",
    "        align_corners=False\n",
    "    )\n",
    "\n",
    "    predicted = upsampled_logits.argmax(dim=1).cpu().numpy()\n",
    "    masks = masks.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(predicted.shape[0]):\n",
    "    print(i)\n",
    "    axarr[i,0].imshow(prediction_to_vis(predicted[i,:,:]))\n",
    "    #axarr[i,1].imshow(prediction_to_vis(masks[i,:,:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
