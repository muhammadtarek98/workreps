{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision\n",
    "import os\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor ,SegformerFeatureExtractor,SegformerConfig\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import albumentations as aug\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import evaluate\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import BasePredictionWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"0\"\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#torch.cuda.memory_summary(device=device, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self,id2label,model_name,label2id,num_classes):\n",
    "        super(Model,self).__init__()\n",
    "        self.id2label=id2label\n",
    "        self.model_name=model_name\n",
    "        self.label2id=label2id\n",
    "        self.num_classes=num_classes\n",
    "        self.model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            self.model_name,\n",
    "            ignore_mismatched_sizes=True,\n",
    "            num_labels=self.num_classes,\n",
    "            id2label=self.id2label,\n",
    "            label2id=self.label2id,\n",
    "            reshape_last_stage=True)\n",
    "        self.model.config.num_labels=self.num_classes\n",
    "        for para in self.model.parameters():\n",
    "            para.requires_grad=True\n",
    "    def forward(self,idx,mask):\n",
    "        output=self.model(idx,mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSegmentationDataset(Dataset):\n",
    "    \"\"\"Image segmentation dataset.\"\"\"\n",
    "    def __init__(self, root_dir, feature_extractor,id2label, transforms=None, train=True):\n",
    "        super(ImageSegmentationDataset,self).__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.id2label =  id2label\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.train = train\n",
    "        self.transforms = transforms\n",
    "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, \"pngmasks\")\n",
    "        image_file_names = []\n",
    "        for root, dirs, files in os.walk(self.img_dir):\n",
    "            image_file_names.extend(files)\n",
    "        self.images = sorted(image_file_names)\n",
    "        # read annotations\n",
    "        annotation_file_names = []\n",
    "        for root, dirs, files in os.walk(self.ann_dir):\n",
    "            annotation_file_names.extend(files)\n",
    "        self.annotations = sorted(annotation_file_names)\n",
    "        assert len(self.images) == len(self.annotations) or len(self.images)==0 ,  \"There must be as many images as there are segmentation maps\"\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(os.path.join(self.img_dir, self.images[idx]))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        segmentation_map = cv2.imread(os.path.join(self.ann_dir, self.annotations[idx]))\n",
    "        segmentation_map = cv2.cvtColor(segmentation_map, cv2.COLOR_BGR2GRAY)\n",
    "        # randomly crop + pad both image and segmentation map to same size\n",
    "        encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors=\"pt\")\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "        return encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSegmentationDatasetInfernce(Dataset):\n",
    "    \"\"\"Image segmentation dataset.\"\"\"\n",
    "    def __init__(self, image_dir,feature_extractor):\n",
    "        super(ImageSegmentationDatasetInfernce,self).__init__()\n",
    "        self.img_dir = image_dir\n",
    "        self.feature_extractor=feature_extractor\n",
    "        image_file_names = []\n",
    "        for root, dirs, files in os.walk(self.img_dir):\n",
    "            image_file_names.extend(files)\n",
    "        self.images = sorted(image_file_names)\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(os.path.join(self.img_dir, self.images[idx]))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        encoded_inputs = self.feature_extractor(image, return_tensors=\"pt\")\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "        return encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegformerFinetuner(pl.LightningModule):\n",
    "    def __init__(self, id2label,model_name, train_dataloader=None, val_dataloader=None, test_dataloader=None, metrics_interval=100):\n",
    "        super(SegformerFinetuner, self).__init__()\n",
    "        self.id2label = id2label\n",
    "        self.metrics_interval = metrics_interval\n",
    "        self.train_dl = train_dataloader\n",
    "        self.val_dl = val_dataloader\n",
    "        self.test_dl = test_dataloader\n",
    "        self.num_classes = len(id2label.keys())\n",
    "        self.label2id = {v: k for k, v in self.id2label.items()}\n",
    "        self.model_name=model_name\n",
    "        self.model =SegformerForSemanticSegmentation.from_pretrained(self.model_name,\n",
    "                                                                    ignore_mismatched_sizes=True,\n",
    "                                                   reshape_last_stage=True)\n",
    "        \n",
    "        self.model_torch_class=Model(id2label=self.id2label,\n",
    "                         model_name=self.model_name,\n",
    "                         label2id=self.label2id,\n",
    "                         num_classes=self.num_classes)\n",
    "        self.train_mean_iou = evaluate.load(\"mean_iou\")\n",
    "        self.val_mean_iou = evaluate.load(\"mean_iou\")\n",
    "        self.test_mean_iou = evaluate.load(\"mean_iou\")\n",
    "    def forward(self, images, masks=None):\n",
    "        outputs = self.model(images,masks)\n",
    "        return outputs\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        outputs = self(images, masks)\n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        upsampled_logits = nn.functional.interpolate(logits, size=masks.shape[-2:], mode=\"nearest-exact\")\n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "        self.train_mean_iou.add_batch(predictions=predicted.detach().cpu().numpy(),\n",
    "                                      references=masks.detach().cpu().numpy())\n",
    "        if batch_nb % self.metrics_interval == 0:\n",
    "            metrics = self.train_mean_iou.compute(num_labels=self.num_classes, ignore_index=255, reduce_labels=False, )\n",
    "            metrics = {'loss': loss, \"mean_iou\": metrics[\"mean_iou\"], \"mean_accuracy\": metrics[\"mean_accuracy\"]}\n",
    "            for k, v in metrics.items():\n",
    "                self.log(k, v,enable_graph=True,prog_bar=True)\n",
    "            self.log_predictions_to_tensorboard(images, masks, predicted, 'train')\n",
    "            return metrics\n",
    "        else:\n",
    "            return {'loss': loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        outputs = self(images, masks)\n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        upsampled_logits = nn.functional.interpolate(logits, size=masks.shape[-2:], mode=\"nearest-exact\")\n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "        self.val_mean_iou.add_batch(predictions=predicted.detach().cpu().numpy(),\n",
    "                                    references=masks.detach().cpu().numpy())\n",
    "        val_metrics = self.val_mean_iou.compute(num_labels=self.num_classes, ignore_index=255, reduce_labels=False)\n",
    "        val_metrics = {'val_loss': loss, \"val_mean_iou\": val_metrics[\"mean_iou\"],\n",
    "                       \"val_mean_accuracy\": val_metrics[\"mean_accuracy\"]}\n",
    "\n",
    "        for k, v in val_metrics.items():\n",
    "            self.log(k, v,enable_graph=True,prog_bar=True)\n",
    "        self.log_predictions_to_tensorboard(images, masks, predicted, 'val')\n",
    "        return val_metrics\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        outputs = self(images, masks)\n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        upsampled_logits = nn.functional.interpolate(logits, size=masks.shape[-2:], mode=\"nearest-exact\")\n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "        self.test_mean_iou.add_batch(predictions=predicted.detach().cpu().numpy(),\n",
    "                                                    references=masks.detach().cpu().numpy())\n",
    "        test_metircs=self.test_mean_iou.compute(num_labels=self.num_classes,ignore_index=255,reduce_labels=False)\n",
    "        test_metircs = {'test_loss': loss, \"test_mean_iou\": test_metircs[\"mean_iou\"],\n",
    "                        \"test_mean_accuracy\": test_metircs[\"mean_accuracy\"]}\n",
    "        for k, v in test_metircs.items():\n",
    "            self.log(k, v,enable_graph=True,prog_bar=True)\n",
    "        self.log_predictions_to_tensorboard(images, masks, predicted, 'test')\n",
    "        return test_metircs     \n",
    "    def configure_optimizers(self):\n",
    "        return AdamW([p for p in self.parameters() if p.requires_grad], lr=2e-06, eps=1e-08)\n",
    "    def train_dataloader(self):\n",
    "        return self.train_dl\n",
    "    def val_dataloader(self):\n",
    "        return self.val_dl\n",
    "    def test_dataloader(self):\n",
    "        return self.test_dl\n",
    "    def log_predictions_to_tensorboard(self, images, masks, predictions, mode='train'):\n",
    "        img_grid = torchvision.utils.make_grid(images)\n",
    "        mask_grid = torchvision.utils.make_grid(masks.unsqueeze(1))  # Assuming masks are single-channel\n",
    "        pred_grid = torchvision.utils.make_grid(predictions.unsqueeze(1))  # Assuming predictions are single-channel\n",
    "        self.logger.experiment.add_image(f'{mode}_images', img_grid, self.current_epoch)\n",
    "        self.logger.experiment.add_image(f'{mode}_masks', mask_grid, self.current_epoch)\n",
    "        self.logger.experiment.add_image(f'{mode}_predictions', pred_grid, self.current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['background', 'stone']\n",
      "{0: 'background', 1: 'stone'}\n",
      "{'background': 0, 'stone': 1}\n"
     ]
    }
   ],
   "source": [
    "classes = [\"background\",\"stone\"]\n",
    "print(classes)\n",
    "id2label = {0: classes[0],1:classes[1]}\n",
    "print(id2label)\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cplus/projects/m.tarek_master/gravel_2D/graval_detection_project/gravel_env/lib/python3.10/site-packages/transformers/models/segformer/image_processing_segformer.py:103: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dir = \"/home/cplus/projects/m.tarek_master/gravel_2D/graval_detection_project/datasets/under_water_masks_dataset/train\"\n",
    "valid_dir = \"/home/cplus/projects/m.tarek_master/gravel_2D/graval_detection_project/datasets/under_water_masks_dataset/val\"\n",
    "test_dir = \"/home/cplus/projects/m.tarek_master/gravel_2D/graval_detection_project/datasets/under_water_masks_dataset/test\"\n",
    "inference_dir=\"/home/cplus/projects/m.tarek_master/graval_detection_project/115351AA.mp4_\"\n",
    "model_name=\"nvidia/mit-b1\"\n",
    "feature_extractor = SegformerImageProcessor.from_pretrained(model_name)\n",
    "feature_extractor.do_reduce_labels = False\n",
    "feature_extractor.size = 1080\n",
    "\n",
    "train_dataset = ImageSegmentationDataset(root_dir=train_dir, feature_extractor=feature_extractor,id2label=id2label, transforms=None, train=True)\n",
    "valid_dataset = ImageSegmentationDataset(root_dir=valid_dir,feature_extractor=feature_extractor,id2label=id2label)\n",
    "test_dataset = ImageSegmentationDataset(root_dir=test_dir,feature_extractor=feature_extractor,id2label=id2label)\n",
    "inference_dataset=ImageSegmentationDatasetInfernce(image_dir=inference_dir,feature_extractor=feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SegformerImageProcessor {\n",
      "  \"_valid_processor_keys\": [\n",
      "    \"images\",\n",
      "    \"segmentation_maps\",\n",
      "    \"do_resize\",\n",
      "    \"size\",\n",
      "    \"resample\",\n",
      "    \"do_rescale\",\n",
      "    \"rescale_factor\",\n",
      "    \"do_normalize\",\n",
      "    \"image_mean\",\n",
      "    \"image_std\",\n",
      "    \"do_reduce_labels\",\n",
      "    \"return_tensors\",\n",
      "    \"data_format\",\n",
      "    \"input_data_format\"\n",
      "  ],\n",
      "  \"do_normalize\": true,\n",
      "  \"do_reduce_labels\": false,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_processor_type\": \"SegformerImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": 1080\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1,shuffle=False,num_workers=0)\n",
    "test_dataloader  = DataLoader(test_dataset,batch_size=1,shuffle=False,num_workers=0)\n",
    "#inference_dataloader=DataLoader(inference_dataset,batch_size=2,shuffle=False,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(save_dir=\"logs\", name=\"segformer_logs_b1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b1 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b1 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SegformerFinetuner(\n",
       "  (model): SegformerForSemanticSegmentation(\n",
       "    (segformer): SegformerModel(\n",
       "      (encoder): SegformerEncoder(\n",
       "        (patch_embeddings): ModuleList(\n",
       "          (0): SegformerOverlapPatchEmbeddings(\n",
       "            (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "            (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SegformerOverlapPatchEmbeddings(\n",
       "            (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): SegformerOverlapPatchEmbeddings(\n",
       "            (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): SegformerOverlapPatchEmbeddings(\n",
       "            (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (block): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): SegformerLayer(\n",
       "              (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SegformerAttention(\n",
       "                (self): SegformerEfficientSelfAttention(\n",
       "                  (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "                  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (output): SegformerSelfOutput(\n",
       "                  (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): SegformerMixFFN(\n",
       "                (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                (dwconv): SegformerDWConv(\n",
       "                  (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                )\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "                (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SegformerLayer(\n",
       "              (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SegformerAttention(\n",
       "                (self): SegformerEfficientSelfAttention(\n",
       "                  (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "                  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (output): SegformerSelfOutput(\n",
       "                  (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SegformerDropPath(p=0.014285714365541935)\n",
       "              (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): SegformerMixFFN(\n",
       "                (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                (dwconv): SegformerDWConv(\n",
       "                  (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                )\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "                (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): ModuleList(\n",
       "            (0): SegformerLayer(\n",
       "              (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SegformerAttention(\n",
       "                (self): SegformerEfficientSelfAttention(\n",
       "                  (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (output): SegformerSelfOutput(\n",
       "                  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SegformerDropPath(p=0.02857142873108387)\n",
       "              (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): SegformerMixFFN(\n",
       "                (dense1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (dwconv): SegformerDWConv(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                )\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "                (dense2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SegformerLayer(\n",
       "              (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SegformerAttention(\n",
       "                (self): SegformerEfficientSelfAttention(\n",
       "                  (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (output): SegformerSelfOutput(\n",
       "                  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SegformerDropPath(p=0.04285714402794838)\n",
       "              (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): SegformerMixFFN(\n",
       "                (dense1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (dwconv): SegformerDWConv(\n",
       "                  (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                )\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "                (dense2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): SegformerLayer(\n",
       "              (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SegformerAttention(\n",
       "                (self): SegformerEfficientSelfAttention(\n",
       "                  (query): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (value): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "                  (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (output): SegformerSelfOutput(\n",
       "                  (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SegformerDropPath(p=0.05714285746216774)\n",
       "              (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): SegformerMixFFN(\n",
       "                (dense1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "                (dwconv): SegformerDWConv(\n",
       "                  (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "                )\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "                (dense2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SegformerLayer(\n",
       "              (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SegformerAttention(\n",
       "                (self): SegformerEfficientSelfAttention(\n",
       "                  (query): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (value): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "                  (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (output): SegformerSelfOutput(\n",
       "                  (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SegformerDropPath(p=0.0714285746216774)\n",
       "              (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): SegformerMixFFN(\n",
       "                (dense1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "                (dwconv): SegformerDWConv(\n",
       "                  (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "                )\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "                (dense2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): SegformerLayer(\n",
       "              (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SegformerAttention(\n",
       "                (self): SegformerEfficientSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SegformerSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SegformerDropPath(p=0.08571428805589676)\n",
       "              (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): SegformerMixFFN(\n",
       "                (dense1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (dwconv): SegformerDWConv(\n",
       "                  (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "                )\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "                (dense2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SegformerLayer(\n",
       "              (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SegformerAttention(\n",
       "                (self): SegformerEfficientSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SegformerSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SegformerDropPath(p=0.10000000149011612)\n",
       "              (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): SegformerMixFFN(\n",
       "                (dense1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (dwconv): SegformerDWConv(\n",
       "                  (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "                )\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "                (dense2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): ModuleList(\n",
       "          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decode_head): SegformerDecodeHead(\n",
       "      (linear_c): ModuleList(\n",
       "        (0): SegformerMLP(\n",
       "          (proj): Linear(in_features=64, out_features=256, bias=True)\n",
       "        )\n",
       "        (1): SegformerMLP(\n",
       "          (proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "        (2): SegformerMLP(\n",
       "          (proj): Linear(in_features=320, out_features=256, bias=True)\n",
       "        )\n",
       "        (3): SegformerMLP(\n",
       "          (proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (linear_fuse): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (classifier): Conv2d(256, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (model_torch_class): Model(\n",
       "    (model): SegformerForSemanticSegmentation(\n",
       "      (segformer): SegformerModel(\n",
       "        (encoder): SegformerEncoder(\n",
       "          (patch_embeddings): ModuleList(\n",
       "            (0): SegformerOverlapPatchEmbeddings(\n",
       "              (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "              (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (1): SegformerOverlapPatchEmbeddings(\n",
       "              (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (2): SegformerOverlapPatchEmbeddings(\n",
       "              (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "              (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (3): SegformerOverlapPatchEmbeddings(\n",
       "              (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (block): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "                    (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "                    (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "                    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (1): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "                    (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "                    (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "                    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SegformerDropPath(p=0.014285714365541935)\n",
       "                (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "                    (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "                    (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "                    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SegformerDropPath(p=0.02857142873108387)\n",
       "                (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (1): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "                    (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "                    (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "                    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SegformerDropPath(p=0.04285714402794838)\n",
       "                (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ModuleList(\n",
       "              (0): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (value): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "                    (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SegformerDropPath(p=0.05714285746216774)\n",
       "                (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (1): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (value): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "                    (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SegformerDropPath(p=0.0714285746216774)\n",
       "                (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (3): ModuleList(\n",
       "              (0): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SegformerDropPath(p=0.08571428805589676)\n",
       "                (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (1): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SegformerDropPath(p=0.10000000149011612)\n",
       "                (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): ModuleList(\n",
       "            (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (decode_head): SegformerDecodeHead(\n",
       "        (linear_c): ModuleList(\n",
       "          (0): SegformerMLP(\n",
       "            (proj): Linear(in_features=64, out_features=256, bias=True)\n",
       "          )\n",
       "          (1): SegformerMLP(\n",
       "            (proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (2): SegformerMLP(\n",
       "            (proj): Linear(in_features=320, out_features=256, bias=True)\n",
       "          )\n",
       "          (3): SegformerMLP(\n",
       "            (proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (linear_fuse): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batch_norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (classifier): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SegformerFineTuner=SegformerFinetuner(id2label=id2label,train_dataloader=train_dataloader,val_dataloader=valid_dataloader,\n",
    "                                    test_dataloader=test_dataloader,metrics_interval=10,model_name=model_name)\n",
    "SegformerFineTuner.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                                                Param #\n",
       "==============================================================================================================\n",
       "SegformerFinetuner                                                                    --\n",
       "SegformerForSemanticSegmentation: 1-1                                               --\n",
       "    SegformerModel: 2-1                                                            --\n",
       "        SegformerEncoder: 3-1                                                     13,151,424\n",
       "    SegformerDecodeHead: 2-2                                                       --\n",
       "        ModuleList: 3-2                                                           263,168\n",
       "        Conv2d: 3-3                                                               262,144\n",
       "        BatchNorm2d: 3-4                                                          512\n",
       "        ReLU: 3-5                                                                 --\n",
       "        Dropout: 3-6                                                              --\n",
       "        Conv2d: 3-7                                                               257,000\n",
       "Model: 1-2                                                                          --\n",
       "    SegformerForSemanticSegmentation: 2-3                                          --\n",
       "        SegformerModel: 3-8                                                       13,151,424\n",
       "        SegformerDecodeHead: 3-9                                                  526,338\n",
       "==============================================================================================================\n",
       "Total params: 27,612,010\n",
       "Trainable params: 27,612,010\n",
       "Non-trainable params: 0\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=SegformerFineTuner,device=SegformerFineTuner.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=100, verbose=False, mode=\"min\")\n",
    "checkpoint_callback = ModelCheckpoint(save_top_k=1, monitor=\"val_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=1, val_check_interval=len(valid_dataloader), accelerator=\"gpu\", devices=1,\n",
    "                     callbacks=[early_stop_callback, checkpoint_callback], logger=logger,enable_progress_bar=True,fast_dev_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/cplus/projects/m.tarek_master/gravel_2D/graval_detection_project/gravel_env/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name              | Type                             | Params\n",
      "-----------------------------------------------------------------------\n",
      "0 | model             | SegformerForSemanticSegmentation | 13.9 M\n",
      "1 | model_torch_class | Model                            | 13.7 M\n",
      "-----------------------------------------------------------------------\n",
      "27.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.6 M    Total params\n",
      "110.448   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cplus/projects/m.tarek_master/gravel_2D/graval_detection_project/gravel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "/home/cplus/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  iou = total_area_intersect / total_area_union\n",
      "/home/cplus/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:260: RuntimeWarning: invalid value encountered in divide\n",
      "  acc = total_area_intersect / total_area_label\n",
      "/home/cplus/projects/m.tarek_master/gravel_2D/graval_detection_project/gravel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83db86aecbfc43a5a001e1bb8beba03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d582c29b1854c369b62c940debdef47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model=SegformerFineTuner,val_dataloaders=valid_dataloader,train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard \n",
    "%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.validate(model=SegformerFineTuner,\n",
    "                 dataloaders=valid_dataloader,\n",
    "                 ckpt_path=\"/home/cplus/projects/m.tarek_master/graval_detection_project/transformers_for_CV/logs/segformer_logs_b1/version_0/checkpoints/epoch=98-step=56455.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=SegformerFineTuner,\n",
    "             ckpt_path=\"/home/cplus/projects/m.tarek_master/graval_detection_project/transformers_for_CV/logs/segformer_logs_b1/version_0/checkpoints/epoch=98-step=56455.ckpt\",\n",
    "             dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.predict(ckpt_path=\"/home/cplus/projects/m.tarek_master/graval_detection_project/transformers_for_CV/logs/segformer_logs_b1/version_0/checkpoints/epoch=98-step=56455.ckpt\",\n",
    "               model=SegformerFineTuner,\n",
    "               dataloaders=inference_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_logits = nn.functional.interpolate(logits, size=(1080,1080), mode=\"nearest-exact\")\n",
    "predicted = upsampled_logits.argmax(dim=0)\n",
    "predicted=predicted.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(obj=SegformerFineTuner.model,f=\"test2922024_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('/home/cplus/projects/m.tarek_master/graval_detection_project/115351AA.mp4_/11_left.jpg')\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = feature_extractor(image, return_tensors=\"pt\").pixel_values.to(\"cpu\")\n",
    "print(pixel_values.shape)\n",
    "print(pixel_values.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ade_palette():\n",
    "    \"\"\"ADE20K palette that maps each class to RGB values.\"\"\"\n",
    "    return [[0,0,0],[255, 255, 255]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = SegformerFineTuner.model(pixel_values=pixel_values)\n",
    "\n",
    "logits = outputs.logits.cpu()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_segmentation_map = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "color_seg = np.zeros((predicted_segmentation_map.shape[0],\n",
    "                      predicted_segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "\n",
    "palette = np.array(ade_palette())\n",
    "for label, color in enumerate(palette):\n",
    "    color_seg[predicted_segmentation_map == label, :] = color\n",
    " #Convert to BGR\n",
    "color_seg = color_seg[..., ::-1]\n",
    "\n",
    "# Show image + mask\n",
    "img = np.array(image) * 0.5 + color_seg * 0.5\n",
    "img = img.astype(np.uint8)\n",
    "\n",
    "#plt.figure(figsize=(15, 10))\n",
    "plt.imshow(predicted_segmentation_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"with torch.no_grad():\n",
    "    for image in inference_dataloader:\n",
    "        outputs = SegformerFineTuner.model(pixel_values=image)\n",
    "        \n",
    "        print(predicted_segmentation_map)\"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"with torch.no_grad():\n",
    "    for idx,image in enumerate(inference_dataloader):\n",
    "      outputs = SegformerFineTuner.model(image)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = {\n",
    "    0:(0,0,0),\n",
    "    1:(0,255,0),\n",
    "}\n",
    "\n",
    "def prediction_to_vis(prediction):\n",
    "    vis_shape = prediction.shape + (3,)\n",
    "    vis = np.zeros(vis_shape)\n",
    "    for i,c in color_map.items():\n",
    "        vis[prediction == i] = color_map[i]\n",
    "    return Image.fromarray(vis.astype(np.uint8))\n",
    "\n",
    "for batch in inference_dataloader:\n",
    "    images, masks = batch['pixel_values']\n",
    "    outputs = SegformerFineTuner.model(images)\n",
    "    loss, logits = outputs[0], outputs[1]\n",
    "    upsampled_logits = nn.functional.interpolate(logits,size=masks.shape[-2:],mode=\"bilinear\",align_corners=False)\n",
    "    predicted_mask = upsampled_logits.argmax(dim=1).cpu().numpy()\n",
    "    masks = masks.cpu().numpy()\n",
    "n_plots = 4\n",
    "\n",
    "f, axarr = plt.subplots(n_plots,2)\n",
    "f.set_figheight(15)\n",
    "f.set_figwidth(15)\n",
    "for i in range(n_plots):\n",
    "    axarr[i,0].imshow(prediction_to_vis(predicted_mask[i,:,:]))\n",
    "    axarr[i,1].imshow(prediction_to_vis(masks[i,:,:]))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = {\n",
    "    0: (0, 0, 0),\n",
    "    1: (255, 255, 255),\n",
    "}\n",
    "\n",
    "def prediction_to_vis(prediction):\n",
    "    vis_shape = prediction.shape + (3,)\n",
    "    vis = np.zeros(vis_shape, dtype=np.uint8)\n",
    "    for i, c in color_map.items():\n",
    "        vis[prediction == i] = color_map[i]\n",
    "    return Image.fromarray(vis)\n",
    "\n",
    "save_dir = \"predicted_masks\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for batch_idx, batch in enumerate(test_dataloader):\n",
    "    images, masks = batch['pixel_values'], batch['labels']\n",
    "    outputs = SegformerFineTuner.model(images, masks)\n",
    "        \n",
    "    loss, logits = outputs[0], outputs[1]\n",
    "\n",
    "    upsampled_logits = nn.functional.interpolate(logits, size=masks.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    predicted = upsampled_logits.argmax(dim=1).cpu().numpy()\n",
    "    masks = masks.cpu().numpy()\n",
    "    print(masks.shape)\n",
    "\n",
    "    \"\"\"for i in range(predicted.shape[0]):\n",
    "        predicted_mask = prediction_to_vis(predicted[i, :, :])\n",
    "        predicted_mask_path = os.path.join(save_dir, f\"batch_{batch_idx}_image_{i}_predicted_mask.png\")\n",
    "        predicted_mask.save(predicted_mask_path)\n",
    "        ground_truth_mask = prediction_to_vis(masks[i, :, :])\n",
    "        ground_truth_mask_path = os.path.join(save_dir, f\"batch_{batch_idx}_image_{i}_ground_truth_mask.png\")\n",
    "        ground_truth_mask.save(ground_truth_mask_path)\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(predicted_mask)\n",
    "        plt.title('Predicted')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(ground_truth_mask)\n",
    "        plt.title('Ground Truth')\n",
    "        plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0ad75e109fa3475da2bb05d9cf879a5a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0e7b22564ad648c9a58ae1ed21e66419": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1943e173254140428cf7aa445ae03085": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "289cac2d76ec4eada4f5a6707893e735": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2ffe9fc8132148ba862d9fdf3f9519ba",
       "style": "IPY_MODEL_b4c3b34c238a456c9c49f4189a6b422e",
       "value": "203/203[09:28&lt;00:00,0.36it/s]"
      }
     },
     "2e188e15bbb8448c8725e3cc3cfdd4c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2ffe9fc8132148ba862d9fdf3f9519ba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3566411d95f743858467fdde5414d77e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "4440eed9a21241c2b690a48f66053ca8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "visibility": "hidden",
       "width": "100%"
      }
     },
     "474064763dfc4346b1f816dfad1fa4bc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "50d1482a8ec249aabd04f317e48040e6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "52662f71ffc04aab86e5cf43d988bf04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_0e7b22564ad648c9a58ae1ed21e66419",
       "style": "IPY_MODEL_77c4455da6e74a2a9e6d9e3d8dd3e361",
       "value": "Epoch0:71%"
      }
     },
     "5d582c29b1854c369b62c940debdef47": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ae569eaa3c85474aa4870b49fef0cc2c",
        "IPY_MODEL_88cde312ac70478393171a00df047724",
        "IPY_MODEL_64b6eaf687444a8fb7dad15aade3e90c"
       ],
       "layout": "IPY_MODEL_e9f561e3565f4a1b9c8280323427cabf"
      }
     },
     "61dd21fb73074a289efec967dcdb5007": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "64b6eaf687444a8fb7dad15aade3e90c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a54292490cab495aa16edf3e7a35ac05",
       "style": "IPY_MODEL_9865dbe7cc0a4598bba9839910e59101",
       "value": "0/203[00:00&lt;?,?it/s]"
      }
     },
     "65134aefe7a6430291afe7ae71896b4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e2a4756f0d83465b90fdaf022fd92192",
       "style": "IPY_MODEL_8db8ae03e4814fd5852fc8f4d89fda07",
       "value": "ValidationDataLoader0:100%"
      }
     },
     "698d4c241ee74fe4af6030841e392b36": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "visibility": "hidden",
       "width": "100%"
      }
     },
     "6b76e80f0ebd4cf38b49a3922bbf70b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_e71df5a59d654f05910aa000ea9ceb18",
       "max": 574,
       "style": "IPY_MODEL_b5fca616e4604a1b91d3d4e4972c5af9",
       "value": 406
      }
     },
     "6da1bdba62804a26a6a09b1166dc2e65": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_9a9e787f7ba744eca29a21b34c580e3c",
       "max": 203,
       "style": "IPY_MODEL_d54734fb82a649c985b7e5064f94088d",
       "value": 203
      }
     },
     "6f623d3382e945888d5a6761146b6c37": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "72052737f4dd4b919c4e6f93ae110082": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "754eea49eec0430a81242bd82b02454d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "770f1d54582147118452721e83b008f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_61dd21fb73074a289efec967dcdb5007",
       "style": "IPY_MODEL_6f623d3382e945888d5a6761146b6c37",
       "value": "406/574[20:41&lt;08:33,0.33it/s,v_num=22,loss=3.330,mean_iou=0.495,mean_accuracy=0.500,val_loss=4.920,val_mean_iou=0.526,val_mean_accuracy=0.527]"
      }
     },
     "77c4455da6e74a2a9e6d9e3d8dd3e361": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "83db86aecbfc43a5a001e1bb8beba03c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_52662f71ffc04aab86e5cf43d988bf04",
        "IPY_MODEL_6b76e80f0ebd4cf38b49a3922bbf70b3",
        "IPY_MODEL_770f1d54582147118452721e83b008f7"
       ],
       "layout": "IPY_MODEL_474064763dfc4346b1f816dfad1fa4bc"
      }
     },
     "88cde312ac70478393171a00df047724": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_8db9c6e948194cb28014d31b194429ee",
       "max": 203,
       "style": "IPY_MODEL_3566411d95f743858467fdde5414d77e"
      }
     },
     "8a9b13b71c984d9d93649d625f6ba0fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_98b6901d37ce478b8cb240c3d14591a9",
       "style": "IPY_MODEL_9ce5afb2086b4be0a7e07e5653357638",
       "value": "SanityCheckingDataLoader0:100%"
      }
     },
     "8db8ae03e4814fd5852fc8f4d89fda07": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8db9c6e948194cb28014d31b194429ee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "9865dbe7cc0a4598bba9839910e59101": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "98b6901d37ce478b8cb240c3d14591a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9a9e787f7ba744eca29a21b34c580e3c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "9ce5afb2086b4be0a7e07e5653357638": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9cfef151faf549ecac2aa233ab30a344": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_50d1482a8ec249aabd04f317e48040e6",
       "max": 2,
       "style": "IPY_MODEL_754eea49eec0430a81242bd82b02454d",
       "value": 2
      }
     },
     "a54292490cab495aa16edf3e7a35ac05": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ae569eaa3c85474aa4870b49fef0cc2c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_0ad75e109fa3475da2bb05d9cf879a5a",
       "style": "IPY_MODEL_2e188e15bbb8448c8725e3cc3cfdd4c0",
       "value": "ValidationDataLoader0:0%"
      }
     },
     "b4c3b34c238a456c9c49f4189a6b422e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b5fca616e4604a1b91d3d4e4972c5af9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d54734fb82a649c985b7e5064f94088d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "df0307d64b474d1d98929a7d2ab53c66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1943e173254140428cf7aa445ae03085",
       "style": "IPY_MODEL_72052737f4dd4b919c4e6f93ae110082",
       "value": "2/2[00:03&lt;00:00,0.56it/s]"
      }
     },
     "e2a4756f0d83465b90fdaf022fd92192": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e71df5a59d654f05910aa000ea9ceb18": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "e9f561e3565f4a1b9c8280323427cabf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
