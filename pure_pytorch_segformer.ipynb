{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor \n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as aug\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, feature_extractor, transforms=None, train=True):\n",
    "        super(CustomDataset,self).__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.train = train\n",
    "        self.transforms = transforms\n",
    "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, \"masks\")\n",
    "        \n",
    "        image_file_names = []\n",
    "        for root, dirs, files in os.walk(self.img_dir):\n",
    "            image_file_names.extend(files)\n",
    "        self.images = sorted(image_file_names)\n",
    "        annotation_file_names = []\n",
    "        for root, dirs, files in os.walk(self.ann_dir):\n",
    "            annotation_file_names.extend(files)\n",
    "        self.annotations = sorted(annotation_file_names)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(os.path.join(self.img_dir, self.images[idx]))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        segmentation_map = cv2.imread(os.path.join(self.ann_dir, self.annotations[idx]))\n",
    "        segmentation_map = cv2.cvtColor(segmentation_map, cv2.COLOR_BGR2GRAY)\n",
    "        if self.transforms is not None:\n",
    "            augmented = self.transforms(image=image, mask=segmentation_map)\n",
    "            encoded_inputs = self.feature_extractor(augmented['image'], augmented['mask'], return_tensors=\"pt\")\n",
    "        else:\n",
    "            encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded_inputs.items():\n",
    "            encoded_inputs[k].squeeze_()\n",
    "\n",
    "        return encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = aug.Compose([\n",
    "    aug.Flip(p=0.5)\n",
    "],is_check_shapes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir =r\"D:\\graval detection project\\datasets\\under_water_masks_dataset\\train\"\n",
    "valid_dir=r\"D:\\graval detection project\\datasets\\under_water_masks_dataset\\val\"\n",
    "test_dir=r\"D:\\graval detection project\\datasets\\under_water_masks_dataset\\test\"\n",
    "feature_extractor = SegformerImageProcessor (align=False, reduce_zero_label=False)\n",
    "train_dataset = CustomDataset(root_dir=train_dir, feature_extractor=feature_extractor, transforms=transform)\n",
    "valid_dataset = CustomDataset(root_dir=valid_dir, feature_extractor=feature_extractor, transforms=None, train=False)\n",
    "test_dataset = CustomDataset(root_dir=test_dir, feature_extractor=feature_extractor, transforms=None, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stone']\n",
      "{0: 'stone'}\n",
      "{'stone': 0}\n"
     ]
    }
   ],
   "source": [
    "classes = [\"stone\"]\n",
    "print(classes)\n",
    "id2label = {0:classes[0]}\n",
    "print(id2label)\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b5 and are newly initialized: ['decode_head.classifier.bias', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_var', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_fuse.weight', 'decode_head.linear_c.0.proj.weight', 'decode_head.classifier.weight', 'decode_head.batch_norm.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_c.2.proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b5\", ignore_mismatched_sizes=True,\n",
    "                                                         num_labels=1, id2label=id2label, label2id=label2id,\n",
    "                                                         reshape_last_stage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for para in model.parameters():\n",
    "    para.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00006)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"Model Initialized!\")\n",
    "print(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                                                      Param #\n",
       "====================================================================================================\n",
       "SegformerForSemanticSegmentation                                            --\n",
       "├─SegformerModel: 1-1                                                       --\n",
       "│    └─SegformerEncoder: 2-1                                                --\n",
       "│    │    └─ModuleList: 3-1                                                 1,929,408\n",
       "│    │    └─ModuleList: 3-2                                                 79,511,552\n",
       "│    │    └─ModuleList: 3-3                                                 2,048\n",
       "├─SegformerDecodeHead: 1-2                                                  --\n",
       "│    └─ModuleList: 2-2                                                      --\n",
       "│    │    └─SegformerMLP: 3-4                                               49,920\n",
       "│    │    └─SegformerMLP: 3-5                                               99,072\n",
       "│    │    └─SegformerMLP: 3-6                                               246,528\n",
       "│    │    └─SegformerMLP: 3-7                                               393,984\n",
       "│    └─Conv2d: 2-3                                                          2,359,296\n",
       "│    └─BatchNorm2d: 2-4                                                     1,536\n",
       "│    └─ReLU: 2-5                                                            --\n",
       "│    └─Dropout: 2-6                                                         --\n",
       "│    └─Conv2d: 2-7                                                          769\n",
       "====================================================================================================\n",
       "Total params: 84,594,113\n",
       "Trainable params: 84,594,113\n",
       "Non-trainable params: 0\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf27a44c0e73468ba590d92ba1334475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(1,11+1):\n",
    "    print(epoch)\n",
    "    progress_bar=tqdm(train_dataloader)\n",
    "    train_accuracies=[]\n",
    "    train_losses=[]\n",
    "    val_accuracies=[]\n",
    "    val_losses=[]\n",
    "    model.train()\n",
    "    for idx,batch in enumerate(progress_bar):\n",
    "        img=batch[\"pixel_values\"].to(device)\n",
    "        seg_map=batch[\"labels\"].to(device)\n",
    "        #reset gradient\n",
    "        optimizer.zero_grad()\n",
    "        #forward pass(prediction)\n",
    "        outputs=model(pixel_values=img,labels=seg_map)\n",
    "        upsampled_logits = torch.nn.functional.interpolate(outputs.logits, size=seg_map.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        pred_seg_map=upsampled_logits.argmax(dim=1)\n",
    "        masks=(seg_map!=255)\n",
    "        pred_seg_map=pred_seg_map[masks].detach().cpu().numpy()\n",
    "        true_seg_map=seg_map[masks].detach().cpu().numpy()\n",
    "        train_accuracy=accuracy_score(y_pred=pred_seg_map,y_true=true_seg_map)\n",
    "        train_loss=outputs.loss\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        train_losses.append(train_loss.item())\n",
    "        progress_bar.set_postfix({'Batch': idx, 'Pixel-wise accuracy': sum(train_accuracies)/len(train_accuracies), 'Loss': sum(train_losses)/len(train_losses)})\n",
    "        train_loss.backward()\n",
    "        #lr_scheduler.step()\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for idx,batch in enumerate(valid_dataloader):\n",
    "                img=batch[\"pixel_values\"].to(device)\n",
    "                seg_map=batch[\"labels\"].to(device)\n",
    "                #reset gradient\n",
    "                optimizer.zero_grad()\n",
    "                #forward pass(prediction)\n",
    "                outputs=model(pixel_values=img,labels=seg_map)\n",
    "                upsampled_logits = torch.nn.functional.interpolate(outputs.logits, size=seg_map.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "                pred_seg_map=upsampled_logits.argmax(dim=1)\n",
    "                \n",
    "                masks=(seg_map!=255)\n",
    "                pred_seg_map=pred_seg_map[masks].detach().cpu().numpy()\n",
    "                true_seg_map=seg_map[masks].detach().cpu().numpy()\n",
    "                val_accuracy=accuracy_score(y_pred=pred_seg_map,y_true=true_seg_map)\n",
    "                val_loss=outputs.loss\n",
    "                val_accuracies.append(val_accuracy)\n",
    "                val_losses.append(val_loss.item())\n",
    "    print(f\"Train Pixel-wise accuracy: {sum(train_accuracies)/len(train_accuracies)}\\\n",
    "         Train Loss: {sum(train_losses)/len(train_losses)}\\\n",
    "         Val Pixel-wise accuracy: {sum(val_accuracies)/len(val_accuracies)}\\\n",
    "         Val Loss: {sum(val_losses)/len(val_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir segformer_pure_pytorch_log/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
