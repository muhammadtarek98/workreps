{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:42:48.800738Z","iopub.status.idle":"2023-12-12T06:42:48.801189Z","shell.execute_reply":"2023-12-12T06:42:48.800977Z","shell.execute_reply.started":"2023-12-12T06:42:48.800955Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from transformers import AdamW\n","import torch\n","from torch import nn\n","from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score\n","from tqdm.notebook import tqdm\n","import os\n","from PIL import Image\n","from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor,SegformerImageProcessor,AdamW\n","import pandas as pd\n","import cv2\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from torchinfo import summary\n","import albumentations as aug\n","import matplotlib.pyplot as plt\n","from torch.nn.functional import binary_cross_entropy_with_logits"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:42:48.803374Z","iopub.status.idle":"2023-12-12T06:42:48.803845Z","shell.execute_reply":"2023-12-12T06:42:48.803610Z","shell.execute_reply.started":"2023-12-12T06:42:48.803588Z"},"trusted":true},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, root_dir, feature_extractor, transforms=None, train=True):\n","        super(CustomDataset,self).__init__()\n","        self.root_dir = root_dir\n","        self.feature_extractor = feature_extractor\n","        self.train = train\n","        self.transforms = transforms\n","        self.img_dir = os.path.join(self.root_dir, \"images\")\n","        self.ann_dir = os.path.join(self.root_dir, \"pngmasks\")\n","        image_file_names = []\n","        for _, _, files in os.walk(self.img_dir):\n","            image_file_names.extend(files)\n","        self.images = sorted(image_file_names)\n","        annotation_file_names = []\n","        for _, _, files in os.walk(self.ann_dir):\n","            annotation_file_names.extend(files)\n","        self.annotations = sorted(annotation_file_names)\n","        assert len(self.images) == len(self.annotations), \"There must be as many images as there are segmentation maps\"\n","    def __len__(self):\n","        return len(self.images)\n","    def __getitem__(self, idx):\n","        image = cv2.imread(os.path.join(self.img_dir, self.images[idx]))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        segmentation_map = cv2.imread(os.path.join(self.ann_dir, self.annotations[idx]))\n","        segmentation_map = cv2.cvtColor(segmentation_map, cv2.COLOR_BGR2GRAY)\n","        if self.transforms is not None:\n","            augmented = self.transforms(image=image, mask=segmentation_map)\n","            encoded_inputs = self.feature_extractor(augmented['image'], augmented['mask'], return_tensors=\"pt\")\n","        else:\n","            encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors=\"pt\")\n","\n","        for k,v in encoded_inputs.items():\n","            encoded_inputs[k].squeeze_()\n","        return encoded_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:42:48.804892Z","iopub.status.idle":"2023-12-12T06:42:48.805368Z","shell.execute_reply":"2023-12-12T06:42:48.805127Z","shell.execute_reply.started":"2023-12-12T06:42:48.805105Z"},"trusted":true},"outputs":[],"source":["transform = aug.Compose([\n","    aug.Flip(p=0.5)\n","    #aug.Normalize(max_pixel_value=255.0,mean=[0.0,0.0,0.0],std=[1.0,1.0,1.0])\n","],is_check_shapes=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:42:48.806644Z","iopub.status.idle":"2023-12-12T06:42:48.807090Z","shell.execute_reply":"2023-12-12T06:42:48.806881Z","shell.execute_reply.started":"2023-12-12T06:42:48.806859Z"},"trusted":true},"outputs":[],"source":["train_dir =r\"D:\\graval detection project\\datasets\\under_water_masks_dataset\\train\"\n","valid_dir=r\"D:\\graval detection project\\datasets\\under_water_masks_dataset\\val\"\n","test_dir=r\"D:\\graval detection project\\datasets\\under_water_masks_dataset\\test\"\n","feature_extractor = SegformerFeatureExtractor (align=False, reduce_zero_label=False,do_rescale=False)\n","train_dataset = CustomDataset(root_dir=train_dir, feature_extractor=feature_extractor, transforms=None)\n","valid_dataset = CustomDataset(root_dir=valid_dir, feature_extractor=feature_extractor, transforms=None, train=False)\n","#test_dataset = CustomDataset(root_dir=test_dir, feature_extractor=feature_extractor, transforms=transform, train=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:42:48.808742Z","iopub.status.idle":"2023-12-12T06:42:48.809186Z","shell.execute_reply":"2023-12-12T06:42:48.808977Z","shell.execute_reply.started":"2023-12-12T06:42:48.808955Z"},"trusted":true},"outputs":[],"source":["train_dataloader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=True)\n","valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=1,shuffle=False)\n","#test_dataloader=DataLoader(dataset=test_dataset,batch_size=1,shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Number of training examples:\", len(train_dataset))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["encoded_inputs = train_dataset[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["encoded_inputs[\"pixel_values\"].shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["encoded_inputs[\"labels\"].shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["encoded_inputs[\"labels\"]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["encoded_inputs[\"labels\"].squeeze().unique()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mask = encoded_inputs[\"labels\"].numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.imshow(mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-12T06:42:49.177502Z","iopub.status.busy":"2023-12-12T06:42:49.177130Z","iopub.status.idle":"2023-12-12T06:42:49.183674Z","shell.execute_reply":"2023-12-12T06:42:49.182685Z","shell.execute_reply.started":"2023-12-12T06:42:49.177472Z"},"trusted":true},"outputs":[],"source":["classes=[\"background\",\"stone\"]\n","id2label = {\n","    classes[0]: (0, 0, 0),    # background pixel\n","    classes[1]: (255, 255, 255)  # stone\n","}\n","label2id = {value: key for key, value in id2label.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b0\", ignore_mismatched_sizes=True,\n","                                                         reshape_last_stage=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-11T18:33:51.671578Z","iopub.status.busy":"2023-12-11T18:33:51.671290Z","iopub.status.idle":"2023-12-11T18:33:51.676823Z","shell.execute_reply":"2023-12-11T18:33:51.675669Z","shell.execute_reply.started":"2023-12-11T18:33:51.671553Z"},"trusted":true},"outputs":[],"source":["for para in model.parameters():\n","    para.requires_grad=True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["summary(model=model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-11T18:33:51.680194Z","iopub.status.busy":"2023-12-11T18:33:51.679769Z","iopub.status.idle":"2023-12-11T18:33:56.507532Z","shell.execute_reply":"2023-12-11T18:33:56.506617Z","shell.execute_reply.started":"2023-12-11T18:33:51.680154Z"},"trusted":true},"outputs":[],"source":["optimizer = AdamW(model.parameters(), lr=0.00006)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","print(\"Model Initialized!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-11T18:33:56.509172Z","iopub.status.busy":"2023-12-11T18:33:56.508801Z"},"trusted":true},"outputs":[],"source":["for epoch in range(1, 11):\n","    print(\"Epoch:\", epoch)\n","    pbar = tqdm(train_dataloader)\n","    accuracies = []\n","    losses = []\n","    val_accuracies = []\n","    val_losses = []\n","    model.train()\n","    for idx, batch in enumerate(pbar):\n","        # get the inputs;\n","        pixel_values = batch[\"pixel_values\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","        # forward\n","        outputs = model(pixel_values=pixel_values, labels=labels)\n","        # evaluate\n","        upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n","        predicted = upsampled_logits.argmax(dim=1)\n","        mask = (labels != 255) # we don't include the background class in the accuracy calculation\n","        pred_labels = predicted[mask].detach().cpu().numpy()\n","        true_labels = labels[mask].detach().cpu().numpy()\n","        accuracy = accuracy_score(pred_labels, true_labels)\n","        loss = outputs.loss\n","        accuracies.append(accuracy)\n","        losses.append(loss.item())\n","        pbar.set_postfix({'Batch': idx, 'Pixel-wise accuracy': sum(accuracies)/len(accuracies), 'Loss': sum(losses)/len(losses)})\n","        # backward + optimize\n","        loss.backward()\n","        optimizer.step()\n","    else:\n","        model.eval()\n","        with torch.no_grad():\n","            for idx, batch in enumerate(train_dataloader):\n","                pixel_values = batch[\"pixel_values\"].to(device)\n","                labels = batch[\"labels\"].to(device)\n","                outputs = model(pixel_values=pixel_values, labels=labels)\n","                upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n","                predicted = upsampled_logits.argmax(dim=1)\n","                mask = (labels != 255) # we don't include the background class in the accuracy calculation\n","                pred_labels = predicted[mask].detach().cpu().numpy()\n","                true_labels = labels[mask].detach().cpu().numpy()\n","                accuracy = accuracy_score(pred_labels, true_labels)\n","                val_loss = outputs.loss\n","                val_accuracies.append(accuracy)\n","                val_losses.append(val_loss.item())\n","    print(f\"Train Pixel-wise accuracy: {sum(accuracies)/len(accuracies)}\\\n","         Train Loss: {sum(losses)/len(losses)}\\\n","         Val Pixel-wise accuracy: {sum(val_accuracies)/len(val_accuracies)}\\\n","         Val Loss: {sum(val_losses)/len(val_losses)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"for epoch in range(1, 11):  # loop over the dataset multiple times\n","    print(\"Epoch:\", epoch)\n","    pbar = tqdm(train_dataloader)\n","    accuracies = []\n","    losses = []\n","    val_accuracies = []\n","    val_losses = []\n","    model.train()\n","\n","    for idx, batch in enumerate(pbar):\n","        # get the inputs;\n","        pixel_values = batch[\"pixel_values\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward\n","        outputs = model(pixel_values=pixel_values, labels=labels)\n","\n","        # calculate binary cross-entropy loss\n","        loss = binary_cross_entropy_with_logits(outputs.logits, labels)\n","\n","        # evaluate\n","        upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n","        predicted = upsampled_logits.argmax(dim=1)\n","        mask = (labels != 255)  # we don't include the background class in the accuracy calculation\n","        pred_labels = predicted[mask].detach().cpu().numpy()\n","        true_labels = labels[mask].detach().cpu().numpy()\n","        accuracy = accuracy_score(pred_labels, true_labels)\n","        accuracies.append(accuracy)\n","        losses.append(loss.item())\n","        pbar.set_postfix({'Batch': idx, 'Pixel-wise accuracy': sum(accuracies) / len(accuracies), 'Loss': sum(losses) / len(losses)})\n","\n","        # backward + optimize\n","        loss.backward()\n","        optimizer.step()\n","\n","    else:\n","        model.eval()\n","        with torch.no_grad():\n","            for idx, batch in enumerate(valid_dataloader):\n","                pixel_values = batch[\"pixel_values\"].to(device)\n","                labels = batch[\"labels\"].to(device)\n","\n","                outputs = model(pixel_values=pixel_values, labels=labels)\n","                upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n","                predicted = upsampled_logits.argmax(dim=1)\n","\n","                mask = (labels != 255)  # we don't include the background class in the accuracy calculation\n","                pred_labels = predicted[mask].detach().cpu().numpy()\n","                true_labels = labels[mask].detach().cpu().numpy()\n","                accuracy = accuracy_score(pred_labels, true_labels)\n","                val_loss = binary_cross_entropy_with_logits(outputs.logits, labels)\n","                val_accuracies.append(accuracy)\n","                val_losses.append(val_loss.item())\n","\n","    print(f\"Train Pixel-wise accuracy: {sum(accuracies) / len(accuracies)}\\\n","         Train Loss: {sum(losses) / len(losses)}\\\n","         Val Pixel-wise accuracy: {sum(val_accuracies) / len(val_accuracies)}\\\n","         Val Loss: {sum(val_losses) / len(val_losses)}\")\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def binary_metrics(pred, target):\n","    # Convert to binary labels (0/1)\n","    pred = (pred > 0.5).float()\n","    target = target.float()\n","    \n","    # Calculate metrics\n","    acc = accuracy_score(pred.cpu().numpy(), target.cpu().numpy())\n","    auc = roc_auc_score(target.cpu().numpy(), pred.cpu().numpy())\n","    recall = recall_score(target.cpu().numpy(), pred.cpu().numpy())\n","    precision = precision_score(target.cpu().numpy(), pred.cpu().numpy())\n","    return acc, auc, recall, precision"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for epoch in range(1, 11):\n","    print(f\"Epoch: {epoch}\")\n","    pbar = tqdm(train_dataloader)\n","    accuracies = []\n","    aucs = []\n","    recalls = []\n","    precisions = []\n","    losses = []    \n","    model.train()\n","    for idx, batch in enumerate(pbar):\n","        # Get inputs and labels\n","        pixel_values = batch[\"pixel_values\"].to(device)\n","        labels = batch[\"labels\"].to(device).float()\n","        # Zero gradients\n","        optimizer.zero_grad()\n","        # Forward pass\n","        outputs = model(pixel_values=pixel_values, labels=labels)\n","        # Calculate binary cross entropy loss\n","        loss = binary_cross_entropy_with_logits(outputs.logits, labels)\n","        # Evaluate and update metrics\n","        acc, auc, recall, precision = binary_metrics(outputs.logits, labels)\n","        accuracies.append(acc)\n","        aucs.append(auc)\n","        recalls.append(recall)\n","        precisions.append(precision)\n","        losses.append(loss.item())\n","        pbar.set_postfix({\n","            \"Batch\": idx,\n","            \"Pixel-wise Accuracy\": sum(accuracies) / len(accuracies),\n","            \"AUC\": sum(aucs) / len(aucs),\n","            \"Recall\": sum(recalls) / len(recalls),\n","            \"Precision\": sum(precisions) / len(precisions),\n","            \"Loss\": sum(losses) / len(losses),\n","        })\n","\n","        # Backward pass and optimize\n","        loss.backward()\n","        optimizer.step()\n","    # Validation loop\n","    with torch.no_grad():\n","        val_accuracies, val_aucs, val_recalls, val_precisions, val_losses = [], [], [], [], []\n","        model.eval()\n","        for idx, batch in enumerate(valid_dataloader):\n","            pixel_values = batch[\"pixel_values\"].to(device)\n","            labels = batch[\"labels\"].to(device).float()\n","            outputs = model(pixel_values=pixel_values, labels=labels)\n","            loss = binary_cross_entropy_with_logits(outputs.logits, labels)\n","            acc, auc, recall, precision = binary_metrics(outputs.logits, labels)\n","            val_accuracies.append(acc)\n","            val_aucs.append(auc)\n","            val_recalls.append(recall)\n","            val_precisions.append(precision)\n","            val_losses.append(loss.item())\n","        print(f\"Train Pixel-wise Accuracy: {sum(accuracies) / len(accuracies)}\\\n","            Train AUC: {sum(aucs) / len(aucs)}\\\n","            Train Recall: {sum(recalls) / len(recalls)}\\\n","            Train Precision: {sum(precisions) / len(precisions)}\\\n","            Train Loss: {sum(losses) / len(losses)}\\n\\\n","            Val Pixel-wise Accuracy: {sum(val_accuracies) / len(val_accuracies)}\\\n","            Val AUC: {sum(val_aucs) / len(val_aucs)}\\\n","            Val Recall: {sum(val_recalls) / len(val_recalls)}\\\n","            Val Precision:{sum(val_precisions) / len(val_precisions)}\\\n","            Val loss:{sum(val_losses) / len(val_losses)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from transformers import AdamW\n","import torch\n","from torch import nn\n","from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score\n","from tqdm.notebook import tqdm\n","import os\n","from PIL import Image\n","from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor,SegformerImageProcessor,AdamW\n","import pandas as pd\n","import cv2\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from torchinfo import summary\n","import albumentations as aug\n","from torch.nn.functional import binary_cross_entropy_with_logits\n","class CustomDataset(Dataset):\n","    def __init__(self, root_dir, feature_extractor, transforms=None, train=True):\n","        super(CustomDataset,self).__init__()\n","        self.root_dir = root_dir\n","        self.feature_extractor = feature_extractor\n","        self.train = train\n","        self.transforms = transforms\n","        self.img_dir = os.path.join(self.root_dir, \"images\")\n","        self.ann_dir = os.path.join(self.root_dir, \"masks\")\n","        image_file_names = []\n","        for root, dirs, files in os.walk(self.img_dir):\n","            image_file_names.extend(files)\n","        self.images = sorted(image_file_names)\n","        annotation_file_names = []\n","        for root, dirs, files in os.walk(self.ann_dir):\n","            annotation_file_names.extend(files)\n","        self.annotations = sorted(annotation_file_names)\n","        assert len(self.images) == len(self.annotations), \"There must be as many images as there are segmentation maps\"\n","    def __len__(self):\n","        return len(self.images)\n","    def __getitem__(self, idx):\n","        image = cv2.imread(os.path.join(self.img_dir, self.images[idx]))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        segmentation_map = cv2.imread(os.path.join(self.ann_dir, self.annotations[idx]))\n","        segmentation_map = cv2.cvtColor(segmentation_map, cv2.COLOR_BGR2GRAY)\n","        if self.transforms is not None:\n","            augmented = self.transforms(image=image, mask=segmentation_map)\n","            encoded_inputs = self.feature_extractor(augmented['image'], augmented['mask'], return_tensors=\"pt\")\n","        else:\n","            encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors=\"pt\")\n","        for k,v in encoded_inputs.items():\n","            encoded_inputs[k].squeeze_()\n","        return encoded_inputs\n","transform = aug.Compose([\n","    #aug.Flip(p=0.5)\n","    aug.Normalize(max_pixel_value=255.0,mean=[0.0,0.0,0.0],std=[1.0,1.0,1.0])\n","],is_check_shapes=False)\n","train_dir =r\"D:\\graval detection project\\datasets\\under_water_masks_dataset\\train\"\n","valid_dir=r\"D:\\graval detection project\\datasets\\under_water_masks_dataset\\val\"\n","test_dir=r\"D:\\graval detection project\\datasets\\under_water_masks_dataset\\test\"\n","feature_extractor = SegformerFeatureExtractor (align=False, reduce_zero_label=False,do_rescale=False)\n","train_dataset = CustomDataset(root_dir=train_dir, feature_extractor=feature_extractor, transforms=transform)\n","valid_dataset = CustomDataset(root_dir=valid_dir, feature_extractor=feature_extractor, transforms=transform, train=False)\n","test_dataset = CustomDataset(root_dir=test_dir, feature_extractor=feature_extractor, transforms=transform, train=False)\n","train_dataloader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=True)\n","valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=1,shuffle=False)\n","test_dataloader=DataLoader(dataset=test_dataset,batch_size=1,shuffle=False)\n","classes = [\"stone\"]\n","print(classes)\n","id2label = {1:classes[0]}\n","print(id2label)\n","label2id = {v: k for k, v in id2label.items()}\n","print(label2id)\n","model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b5\", ignore_mismatched_sizes=True,\n","                                                         num_labels=len(classes),id2label=id2label,label2id=label2id,\n","                                                         reshape_last_stage=True)\n","for para in model.parameters():\n","    para.requires_grad=True\n","summary(model=model)\n","optimizer = AdamW(model.parameters(), lr=0.00006)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","print(\"Model Initialized!\")\n","for epoch in range(1, 11):  # loop over the dataset multiple times\n","    print(\"Epoch:\", epoch)\n","    pbar = tqdm(train_dataloader)\n","    accuracies = []\n","    losses = []\n","    val_accuracies = []\n","    val_losses = []\n","    model.train()\n","    for idx, batch in enumerate(pbar):\n","        # get the inputs;\n","        pixel_values = batch[\"pixel_values\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","        # forward\n","        outputs = model(pixel_values=pixel_values, labels=labels)\n","        # evaluate\n","        upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n","        predicted = upsampled_logits.argmax(dim=1.0)\n","        mask = (labels != 1) # we don't include the background class in the accuracy calculation\n","        pred_labels = predicted[mask].detach().cpu().numpy()\n","        true_labels = labels[mask].detach().cpu().numpy()\n","        accuracy = accuracy_score(pred_labels, true_labels)\n","        loss = outputs.loss\n","        accuracies.append(accuracy)\n","        losses.append(loss.item())\n","        pbar.set_postfix({'Batch': idx, 'Pixel-wise accuracy': sum(accuracies)/len(accuracies), 'Loss': sum(losses)/len(losses)})\n","        # backward + optimize\n","        loss.backward()\n","        optimizer.step()\n","    else:\n","        model.eval()\n","        with torch.no_grad():\n","            for idx, batch in enumerate(valid_dataloader):\n","                pixel_values = batch[\"pixel_values\"].to(device)\n","                labels = batch[\"labels\"].to(device)\n","                outputs = model(pixel_values=pixel_values, labels=labels)\n","                upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n","                predicted = upsampled_logits.argmax(dim=1)\n","                mask = (labels != 1.0) # we don't include the background class in the accuracy calculation\n","                pred_labels = predicted[mask].detach().cpu().numpy()\n","                true_labels = labels[mask].detach().cpu().numpy()\n","                accuracy = accuracy_score(pred_labels, true_labels)\n","                val_loss = outputs.loss\n","                val_accuracies.append(accuracy)\n","                val_losses.append(val_loss.item())\n","    print(f\"Train Pixel-wise accuracy: {sum(accuracies)/len(accuracies)}\\\n","         Train Loss: {sum(losses)/len(losses)}\\\n","         Val Pixel-wise accuracy: {sum(val_accuracies)/len(val_accuracies)}\\\n","         Val Loss: {sum(val_losses)/len(val_losses)}\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4146186,"sourceId":7175133,"sourceType":"datasetVersion"}],"dockerImageVersionId":30616,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
