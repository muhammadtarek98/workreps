{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:42:48.800738Z","iopub.status.idle":"2023-12-12T06:42:48.801189Z","shell.execute_reply":"2023-12-12T06:42:48.800977Z","shell.execute_reply.started":"2023-12-12T06:42:48.800955Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from transformers import AdamW\n","import torch\n","from torch import nn\n","from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score\n","from tqdm.notebook import tqdm\n","import os\n","from PIL import Image\n","from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor,SegformerImageProcessor,AdamW\n","import pandas as pd\n","import cv2\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from torchinfo import summary\n","import albumentations as aug\n","import matplotlib.pyplot as plt\n","from torch.nn.functional import binary_cross_entropy_with_logits"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:42:48.803374Z","iopub.status.idle":"2023-12-12T06:42:48.803845Z","shell.execute_reply":"2023-12-12T06:42:48.803610Z","shell.execute_reply.started":"2023-12-12T06:42:48.803588Z"},"trusted":true},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, root_dir, feature_extractor, transforms=None, train=True):\n","        super(CustomDataset,self).__init__()\n","        self.root_dir = root_dir\n","        self.feature_extractor = feature_extractor\n","        self.train = train\n","        self.transforms = transforms\n","        self.img_dir = os.path.join(self.root_dir, \"images\")\n","        self.ann_dir = os.path.join(self.root_dir, \"pngmasks\")\n","        image_file_names = []\n","        for _, _, files in os.walk(self.img_dir):\n","            image_file_names.extend(files)\n","        self.images = sorted(image_file_names)\n","        annotation_file_names = []\n","        for _, _, files in os.walk(self.ann_dir):\n","            annotation_file_names.extend(files)\n","        self.annotations = sorted(annotation_file_names)\n","        assert len(self.images) == len(self.annotations), \"There must be as many images as there are segmentation maps\"\n","    def __len__(self):\n","        return len(self.images)\n","    def __getitem__(self, idx):\n","        image = cv2.imread(os.path.join(self.img_dir, self.images[idx]))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        segmentation_map = cv2.imread(os.path.join(self.ann_dir, self.annotations[idx]))\n","        segmentation_map = cv2.cvtColor(segmentation_map, cv2.COLOR_BGR2GRAY)\n","        if self.transforms is not None:\n","            augmented = self.transforms(image=image, mask=segmentation_map)\n","            encoded_inputs = self.feature_extractor(augmented['image'], augmented['mask'], return_tensors=\"pt\")\n","        else:\n","            encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors=\"pt\")\n","\n","        for k,v in encoded_inputs.items():\n","            encoded_inputs[k].squeeze_()\n","        return encoded_inputs"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:42:48.804892Z","iopub.status.idle":"2023-12-12T06:42:48.805368Z","shell.execute_reply":"2023-12-12T06:42:48.805127Z","shell.execute_reply.started":"2023-12-12T06:42:48.805105Z"},"trusted":true},"outputs":[],"source":["transform = aug.Compose([\n","    aug.Flip(p=0.5)\n","    #aug.Normalize(max_pixel_value=255.0,mean=[0.0,0.0,0.0],std=[1.0,1.0,1.0])\n","],is_check_shapes=False)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:42:48.806644Z","iopub.status.idle":"2023-12-12T06:42:48.807090Z","shell.execute_reply":"2023-12-12T06:42:48.806881Z","shell.execute_reply.started":"2023-12-12T06:42:48.806859Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Program Files\\Python310\\lib\\site-packages\\transformers\\models\\segformer\\feature_extraction_segformer.py:28: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n","  warnings.warn(\n"]}],"source":["train_dir =r\"D:\\graval detection project\\datasets\\under_water_masks_dataset\\train\"\n","valid_dir=r\"D:\\graval detection project\\datasets\\under_water_masks_dataset\\val\"\n","test_dir=r\"D:\\graval detection project\\datasets\\under_water_masks_dataset\\test\"\n","feature_extractor = SegformerFeatureExtractor (align=False, reduce_zero_label=False,do_rescale=False)\n","train_dataset = CustomDataset(root_dir=train_dir, feature_extractor=feature_extractor, transforms=None)\n","valid_dataset = CustomDataset(root_dir=valid_dir, feature_extractor=feature_extractor, transforms=None, train=False)\n","#test_dataset = CustomDataset(root_dir=test_dir, feature_extractor=feature_extractor, transforms=transform, train=False)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:42:48.808742Z","iopub.status.idle":"2023-12-12T06:42:48.809186Z","shell.execute_reply":"2023-12-12T06:42:48.808977Z","shell.execute_reply.started":"2023-12-12T06:42:48.808955Z"},"trusted":true},"outputs":[],"source":["train_dataloader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=True)\n","valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=1,shuffle=False)\n","#test_dataloader=DataLoader(dataset=test_dataset,batch_size=1,shuffle=False)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of training examples: 580\n"]}],"source":["print(\"Number of training examples:\", len(train_dataset))"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["encoded_inputs = train_dataset[0]"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([3, 512, 512])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["encoded_inputs[\"pixel_values\"].shape"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([512, 512])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["encoded_inputs[\"labels\"].shape"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0]])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["encoded_inputs[\"labels\"]"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([  0, 255])"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["encoded_inputs[\"labels\"].squeeze().unique()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["mask = encoded_inputs[\"labels\"].numpy()"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["<matplotlib.image.AxesImage at 0x17aaf237010>"]},"execution_count":13,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plt.imshow(mask)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-12-12T06:42:49.177502Z","iopub.status.busy":"2023-12-12T06:42:49.177130Z","iopub.status.idle":"2023-12-12T06:42:49.183674Z","shell.execute_reply":"2023-12-12T06:42:49.182685Z","shell.execute_reply.started":"2023-12-12T06:42:49.177472Z"},"trusted":true},"outputs":[],"source":["classes=[\"background\",\"stone\"]\n","id2label = {\n","    classes[0]: (0, 0, 0),    # background pixel\n","    classes[1]: (255, 255, 255)  # stone\n","}\n","label2id = {value: key for key, value in id2label.items()}"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.linear_c.3.proj.weight', 'decode_head.batch_norm.running_mean', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.classifier.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_fuse.weight', 'decode_head.batch_norm.bias', 'decode_head.classifier.bias', 'decode_head.linear_c.2.proj.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_var', 'decode_head.linear_c.1.proj.weight', 'decode_head.batch_norm.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b0\", ignore_mismatched_sizes=True,\n","                                                         reshape_last_stage=True)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-12-11T18:33:51.671578Z","iopub.status.busy":"2023-12-11T18:33:51.671290Z","iopub.status.idle":"2023-12-11T18:33:51.676823Z","shell.execute_reply":"2023-12-11T18:33:51.675669Z","shell.execute_reply.started":"2023-12-11T18:33:51.671553Z"},"trusted":true},"outputs":[],"source":["for para in model.parameters():\n","    para.requires_grad=True"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["====================================================================================================\n","Layer (type:depth-idx)                                                      Param #\n","====================================================================================================\n","SegformerForSemanticSegmentation                                            --\n","├─SegformerModel: 1-1                                                       --\n","│    └─SegformerEncoder: 2-1                                                --\n","│    │    └─ModuleList: 3-1                                                 485,472\n","│    │    └─ModuleList: 3-2                                                 2,832,896\n","│    │    └─ModuleList: 3-3                                                 1,024\n","├─SegformerDecodeHead: 1-2                                                  --\n","│    └─ModuleList: 2-2                                                      --\n","│    │    └─SegformerMLP: 3-4                                               8,448\n","│    │    └─SegformerMLP: 3-5                                               16,640\n","│    │    └─SegformerMLP: 3-6                                               41,216\n","│    │    └─SegformerMLP: 3-7                                               65,792\n","│    └─Conv2d: 2-3                                                          262,144\n","│    └─BatchNorm2d: 2-4                                                     512\n","│    └─ReLU: 2-5                                                            --\n","│    └─Dropout: 2-6                                                         --\n","│    └─Conv2d: 2-7                                                          257,000\n","====================================================================================================\n","Total params: 3,971,144\n","Trainable params: 3,971,144\n","Non-trainable params: 0\n","===================================================================================================="]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["summary(model=model)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-12-11T18:33:51.680194Z","iopub.status.busy":"2023-12-11T18:33:51.679769Z","iopub.status.idle":"2023-12-11T18:33:56.507532Z","shell.execute_reply":"2023-12-11T18:33:56.506617Z","shell.execute_reply.started":"2023-12-11T18:33:51.680154Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Program Files\\Python310\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Model Initialized!\n"]}],"source":["optimizer = AdamW(model.parameters(), lr=0.00006)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","print(\"Model Initialized!\")"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-12-11T18:33:56.509172Z","iopub.status.busy":"2023-12-11T18:33:56.508801Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8338cbc344f44497a27112032bc9dd6a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/580 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[19], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m val_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# get the inputs;\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\tqdm\\notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n","File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[1;32mIn[2], line 24\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     22\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[idx]))\n\u001b[0;32m     23\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 24\u001b[0m segmentation_map \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mann_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mannotations\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m segmentation_map \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(segmentation_map, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for epoch in range(1, 11):\n","    print(\"Epoch:\", epoch)\n","    pbar = tqdm(train_dataloader)\n","    accuracies = []\n","    losses = []\n","    val_accuracies = []\n","    val_losses = []\n","    model.train()\n","    for idx, batch in enumerate(pbar):\n","        # get the inputs;\n","        pixel_values = batch[\"pixel_values\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","        # forward\n","        outputs = model(pixel_values=pixel_values, labels=labels)\n","        # evaluate\n","        upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n","        predicted = upsampled_logits.argmax(dim=1)\n","        mask = (labels != 255) # we don't include the background class in the accuracy calculation\n","        pred_labels = predicted[mask].detach().cpu().numpy()\n","        true_labels = labels[mask].detach().cpu().numpy()\n","        accuracy = accuracy_score(pred_labels, true_labels)\n","        loss = outputs.loss\n","        accuracies.append(accuracy)\n","        losses.append(loss.item())\n","        pbar.set_postfix({'Batch': idx, 'Pixel-wise accuracy': sum(accuracies)/len(accuracies), 'Loss': sum(losses)/len(losses)})\n","        # backward + optimize\n","        loss.backward()\n","        optimizer.step()\n","    else:\n","        model.eval()\n","        with torch.no_grad():\n","            for idx, batch in enumerate(train_dataloader):\n","                pixel_values = batch[\"pixel_values\"].to(device)\n","                labels = batch[\"labels\"].to(device)\n","                outputs = model(pixel_values=pixel_values, labels=labels)\n","                upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n","                predicted = upsampled_logits.argmax(dim=1)\n","                mask = (labels != 255) # we don't include the background class in the accuracy calculation\n","                pred_labels = predicted[mask].detach().cpu().numpy()\n","                true_labels = labels[mask].detach().cpu().numpy()\n","                accuracy = accuracy_score(pred_labels, true_labels)\n","                val_loss = outputs.loss\n","                val_accuracies.append(accuracy)\n","                val_losses.append(val_loss.item())\n","    print(f\"Train Pixel-wise accuracy: {sum(accuracies)/len(accuracies)}\\\n","         Train Loss: {sum(losses)/len(losses)}\\\n","         Val Pixel-wise accuracy: {sum(val_accuracies)/len(val_accuracies)}\\\n","         Val Loss: {sum(val_losses)/len(val_losses)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"for epoch in range(1, 11):  # loop over the dataset multiple times\n","    print(\"Epoch:\", epoch)\n","    pbar = tqdm(train_dataloader)\n","    accuracies = []\n","    losses = []\n","    val_accuracies = []\n","    val_losses = []\n","    model.train()\n","\n","    for idx, batch in enumerate(pbar):\n","        # get the inputs;\n","        pixel_values = batch[\"pixel_values\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward\n","        outputs = model(pixel_values=pixel_values, labels=labels)\n","\n","        # calculate binary cross-entropy loss\n","        loss = binary_cross_entropy_with_logits(outputs.logits, labels)\n","\n","        # evaluate\n","        upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n","        predicted = upsampled_logits.argmax(dim=1)\n","        mask = (labels != 255)  # we don't include the background class in the accuracy calculation\n","        pred_labels = predicted[mask].detach().cpu().numpy()\n","        true_labels = labels[mask].detach().cpu().numpy()\n","        accuracy = accuracy_score(pred_labels, true_labels)\n","        accuracies.append(accuracy)\n","        losses.append(loss.item())\n","        pbar.set_postfix({'Batch': idx, 'Pixel-wise accuracy': sum(accuracies) / len(accuracies), 'Loss': sum(losses) / len(losses)})\n","\n","        # backward + optimize\n","        loss.backward()\n","        optimizer.step()\n","\n","    else:\n","        model.eval()\n","        with torch.no_grad():\n","            for idx, batch in enumerate(valid_dataloader):\n","                pixel_values = batch[\"pixel_values\"].to(device)\n","                labels = batch[\"labels\"].to(device)\n","\n","                outputs = model(pixel_values=pixel_values, labels=labels)\n","                upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n","                predicted = upsampled_logits.argmax(dim=1)\n","\n","                mask = (labels != 255)  # we don't include the background class in the accuracy calculation\n","                pred_labels = predicted[mask].detach().cpu().numpy()\n","                true_labels = labels[mask].detach().cpu().numpy()\n","                accuracy = accuracy_score(pred_labels, true_labels)\n","                val_loss = binary_cross_entropy_with_logits(outputs.logits, labels)\n","                val_accuracies.append(accuracy)\n","                val_losses.append(val_loss.item())\n","\n","    print(f\"Train Pixel-wise accuracy: {sum(accuracies) / len(accuracies)}\\\n","         Train Loss: {sum(losses) / len(losses)}\\\n","         Val Pixel-wise accuracy: {sum(val_accuracies) / len(val_accuracies)}\\\n","         Val Loss: {sum(val_losses) / len(val_losses)}\")\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def binary_metrics(pred, target):\n","    # Convert to binary labels (0/1)\n","    pred = (pred > 0.5).float()\n","    target = target.float()\n","    \n","    # Calculate metrics\n","    acc = accuracy_score(pred.cpu().numpy(), target.cpu().numpy())\n","    auc = roc_auc_score(target.cpu().numpy(), pred.cpu().numpy())\n","    recall = recall_score(target.cpu().numpy(), pred.cpu().numpy())\n","    precision = precision_score(target.cpu().numpy(), pred.cpu().numpy())\n","    return acc, auc, recall, precision"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for epoch in range(1, 11):\n","    print(f\"Epoch: {epoch}\")\n","    pbar = tqdm(train_dataloader)\n","    accuracies = []\n","    aucs = []\n","    recalls = []\n","    precisions = []\n","    losses = []    \n","    model.train()\n","    for idx, batch in enumerate(pbar):\n","        # Get inputs and labels\n","        pixel_values = batch[\"pixel_values\"].to(device)\n","        labels = batch[\"labels\"].to(device).float()\n","        # Zero gradients\n","        optimizer.zero_grad()\n","        # Forward pass\n","        outputs = model(pixel_values=pixel_values, labels=labels)\n","        # Calculate binary cross entropy loss\n","        loss = binary_cross_entropy_with_logits(outputs.logits, labels)\n","        # Evaluate and update metrics\n","        acc, auc, recall, precision = binary_metrics(outputs.logits, labels)\n","        accuracies.append(acc)\n","        aucs.append(auc)\n","        recalls.append(recall)\n","        precisions.append(precision)\n","        losses.append(loss.item())\n","        pbar.set_postfix({\n","            \"Batch\": idx,\n","            \"Pixel-wise Accuracy\": sum(accuracies) / len(accuracies),\n","            \"AUC\": sum(aucs) / len(aucs),\n","            \"Recall\": sum(recalls) / len(recalls),\n","            \"Precision\": sum(precisions) / len(precisions),\n","            \"Loss\": sum(losses) / len(losses),\n","        })\n","\n","        # Backward pass and optimize\n","        loss.backward()\n","        optimizer.step()\n","    # Validation loop\n","    with torch.no_grad():\n","        val_accuracies, val_aucs, val_recalls, val_precisions, val_losses = [], [], [], [], []\n","        model.eval()\n","        for idx, batch in enumerate(valid_dataloader):\n","            pixel_values = batch[\"pixel_values\"].to(device)\n","            labels = batch[\"labels\"].to(device).float()\n","            outputs = model(pixel_values=pixel_values, labels=labels)\n","            loss = binary_cross_entropy_with_logits(outputs.logits, labels)\n","            acc, auc, recall, precision = binary_metrics(outputs.logits, labels)\n","            val_accuracies.append(acc)\n","            val_aucs.append(auc)\n","            val_recalls.append(recall)\n","            val_precisions.append(precision)\n","            val_losses.append(loss.item())\n","        print(f\"Train Pixel-wise Accuracy: {sum(accuracies) / len(accuracies)}\\\n","            Train AUC: {sum(aucs) / len(aucs)}\\\n","            Train Recall: {sum(recalls) / len(recalls)}\\\n","            Train Precision: {sum(precisions) / len(precisions)}\\\n","            Train Loss: {sum(losses) / len(losses)}\\n\\\n","            Val Pixel-wise Accuracy: {sum(val_accuracies) / len(val_accuracies)}\\\n","            Val AUC: {sum(val_aucs) / len(val_aucs)}\\\n","            Val Recall: {sum(val_recalls) / len(val_recalls)}\\\n","            Val Precision:{sum(val_precisions) / len(val_precisions)}\\\n","            Val loss:{sum(val_losses) / len(val_losses)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from transformers import AdamW\n","import torch\n","from torch import nn\n","from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score\n","from tqdm.notebook import tqdm\n","import os\n","from PIL import Image\n","from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor,SegformerImageProcessor,AdamW\n","import pandas as pd\n","import cv2\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from torchinfo import summary\n","import albumentations as aug\n","from torch.nn.functional import binary_cross_entropy_with_logits\n","class CustomDataset(Dataset):\n","    def __init__(self, root_dir, feature_extractor, transforms=None, train=True):\n","        super(CustomDataset,self).__init__()\n","        self.root_dir = root_dir\n","        self.feature_extractor = feature_extractor\n","        self.train = train\n","        self.transforms = transforms\n","        self.img_dir = os.path.join(self.root_dir, \"images\")\n","        self.ann_dir = os.path.join(self.root_dir, \"masks\")\n","        image_file_names = []\n","        for root, dirs, files in os.walk(self.img_dir):\n","            image_file_names.extend(files)\n","        self.images = sorted(image_file_names)\n","        annotation_file_names = []\n","        for root, dirs, files in os.walk(self.ann_dir):\n","            annotation_file_names.extend(files)\n","        self.annotations = sorted(annotation_file_names)\n","        assert len(self.images) == len(self.annotations), \"There must be as many images as there are segmentation maps\"\n","    def __len__(self):\n","        return len(self.images)\n","    def __getitem__(self, idx):\n","        image = cv2.imread(os.path.join(self.img_dir, self.images[idx]))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        segmentation_map = cv2.imread(os.path.join(self.ann_dir, self.annotations[idx]))\n","        segmentation_map = cv2.cvtColor(segmentation_map, cv2.COLOR_BGR2GRAY)\n","        if self.transforms is not None:\n","            augmented = self.transforms(image=image, mask=segmentation_map)\n","            encoded_inputs = self.feature_extractor(augmented['image'], augmented['mask'], return_tensors=\"pt\")\n","        else:\n","            encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors=\"pt\")\n","        for k,v in encoded_inputs.items():\n","            encoded_inputs[k].squeeze_()\n","        return encoded_inputs\n","transform = aug.Compose([\n","    #aug.Flip(p=0.5)\n","    aug.Normalize(max_pixel_value=255.0,mean=[0.0,0.0,0.0],std=[1.0,1.0,1.0])\n","],is_check_shapes=False)\n","train_dir =r\"D:\\graval detection project\\datasets\\under_water_masks_dataset\\train\"\n","valid_dir=r\"D:\\graval detection project\\datasets\\under_water_masks_dataset\\val\"\n","test_dir=r\"D:\\graval detection project\\datasets\\under_water_masks_dataset\\test\"\n","feature_extractor = SegformerFeatureExtractor (align=False, reduce_zero_label=False,do_rescale=False)\n","train_dataset = CustomDataset(root_dir=train_dir, feature_extractor=feature_extractor, transforms=transform)\n","valid_dataset = CustomDataset(root_dir=valid_dir, feature_extractor=feature_extractor, transforms=transform, train=False)\n","test_dataset = CustomDataset(root_dir=test_dir, feature_extractor=feature_extractor, transforms=transform, train=False)\n","train_dataloader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=True)\n","valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=1,shuffle=False)\n","test_dataloader=DataLoader(dataset=test_dataset,batch_size=1,shuffle=False)\n","classes = [\"stone\"]\n","print(classes)\n","id2label = {1:classes[0]}\n","print(id2label)\n","label2id = {v: k for k, v in id2label.items()}\n","print(label2id)\n","model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b5\", ignore_mismatched_sizes=True,\n","                                                         num_labels=len(classes),id2label=id2label,label2id=label2id,\n","                                                         reshape_last_stage=True)\n","for para in model.parameters():\n","    para.requires_grad=True\n","summary(model=model)\n","optimizer = AdamW(model.parameters(), lr=0.00006)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","print(\"Model Initialized!\")\n","for epoch in range(1, 11):  # loop over the dataset multiple times\n","    print(\"Epoch:\", epoch)\n","    pbar = tqdm(train_dataloader)\n","    accuracies = []\n","    losses = []\n","    val_accuracies = []\n","    val_losses = []\n","    model.train()\n","    for idx, batch in enumerate(pbar):\n","        # get the inputs;\n","        pixel_values = batch[\"pixel_values\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","        # forward\n","        outputs = model(pixel_values=pixel_values, labels=labels)\n","        # evaluate\n","        upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n","        predicted = upsampled_logits.argmax(dim=1.0)\n","        mask = (labels != 1) # we don't include the background class in the accuracy calculation\n","        pred_labels = predicted[mask].detach().cpu().numpy()\n","        true_labels = labels[mask].detach().cpu().numpy()\n","        accuracy = accuracy_score(pred_labels, true_labels)\n","        loss = outputs.loss\n","        accuracies.append(accuracy)\n","        losses.append(loss.item())\n","        pbar.set_postfix({'Batch': idx, 'Pixel-wise accuracy': sum(accuracies)/len(accuracies), 'Loss': sum(losses)/len(losses)})\n","        # backward + optimize\n","        loss.backward()\n","        optimizer.step()\n","    else:\n","        model.eval()\n","        with torch.no_grad():\n","            for idx, batch in enumerate(valid_dataloader):\n","                pixel_values = batch[\"pixel_values\"].to(device)\n","                labels = batch[\"labels\"].to(device)\n","                outputs = model(pixel_values=pixel_values, labels=labels)\n","                upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n","                predicted = upsampled_logits.argmax(dim=1)\n","                mask = (labels != 1.0) # we don't include the background class in the accuracy calculation\n","                pred_labels = predicted[mask].detach().cpu().numpy()\n","                true_labels = labels[mask].detach().cpu().numpy()\n","                accuracy = accuracy_score(pred_labels, true_labels)\n","                val_loss = outputs.loss\n","                val_accuracies.append(accuracy)\n","                val_losses.append(val_loss.item())\n","    print(f\"Train Pixel-wise accuracy: {sum(accuracies)/len(accuracies)}\\\n","         Train Loss: {sum(losses)/len(losses)}\\\n","         Val Pixel-wise accuracy: {sum(val_accuracies)/len(val_accuracies)}\\\n","         Val Loss: {sum(val_losses)/len(val_losses)}\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4146186,"sourceId":7175133,"sourceType":"datasetVersion"}],"dockerImageVersionId":30616,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
