{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor ,SegformerFeatureExtractor\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as aug\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSegmentationDataset(Dataset):\n",
    "    \"\"\"Image segmentation dataset.\"\"\"\n",
    "    def __init__(self, root_dir, feature_extractor, transforms=None, train=True):\n",
    "        super(ImageSegmentationDataset,self).__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.train = train\n",
    "        self.transforms = transforms\n",
    "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, \"masks\")\n",
    "        image_file_names = []\n",
    "        for root, dirs, files in os.walk(self.img_dir):\n",
    "            image_file_names.extend(files)\n",
    "        self.images = sorted(image_file_names)\n",
    "        \n",
    "        # read annotations\n",
    "        annotation_file_names = []\n",
    "        for root, dirs, files in os.walk(self.ann_dir):\n",
    "            annotation_file_names.extend(files)\n",
    "        self.annotations = sorted(annotation_file_names)\n",
    "\n",
    "        assert len(self.images) == len(self.annotations), \"There must be as many images as there are segmentation maps\"\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = cv2.imread(os.path.join(self.img_dir, self.images[idx]))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        segmentation_map = cv2.imread(os.path.join(self.ann_dir, self.annotations[idx]))\n",
    "        segmentation_map = cv2.cvtColor(segmentation_map, cv2.COLOR_BGR2GRAY)\n",
    "        if self.transforms is not None:\n",
    "            augmented = self.transforms(image=image, mask=segmentation_map)\n",
    "            encoded_inputs = self.feature_extractor(augmented['image'], augmented['mask'], return_tensors=\"pt\")\n",
    "        else:\n",
    "            encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded_inputs.items():\n",
    "            encoded_inputs[k].squeeze_()\n",
    "\n",
    "        return encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = aug.Compose([\n",
    "    aug.Flip(p=0.5)\n",
    "],is_check_shapes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir =r\"D:\\graval detection project\\datasets\\under_water_masks_dataset\\train\"\n",
    "valid_dir=r\"D:\\graval detection project\\datasets\\under_water_masks_dataset\\val\"\n",
    "test_dir=r\"D:\\graval detection project\\datasets\\under_water_masks_dataset\\test\"\n",
    "feature_extractor = SegformerImageProcessor.from_pretrained (\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "train_dataset = ImageSegmentationDataset(root_dir=train_dir, feature_extractor=feature_extractor, transforms=transform)\n",
    "valid_dataset = ImageSegmentationDataset(root_dir=valid_dir, feature_extractor=feature_extractor, transforms=None, train=False)\n",
    "test_dataset = ImageSegmentationDataset(root_dir=test_dir, feature_extractor=feature_extractor, transforms=None, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"stone\"]\n",
    "print(classes)\n",
    "id2label = {0:classes[0]}\n",
    "print(id2label)\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True,num_workers=0)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=4,shuffle=False,num_workers=0)\n",
    "test_dataloader  = DataLoader(test_dataset,batch_size=4,shuffle=False,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegformerFinetuner(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, id2label, train_dataloader=None, val_dataloader=None, test_dataloader=None, metrics_interval=100):\n",
    "        super(SegformerFinetuner, self).__init__()\n",
    "        self.id2label = id2label\n",
    "        self.metrics_interval = metrics_interval\n",
    "        self.train_dl = train_dataloader\n",
    "        self.val_dl = val_dataloader\n",
    "        self.test_dl = test_dataloader\n",
    "        self.num_classes = len(id2label.keys())\n",
    "        self.label2id = {v:k for k,v in self.id2label.items()}\n",
    "        self.model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\", return_dict=False, num_labels=self.num_classes,id2label=self.id2label,label2id=self.label2id,ignore_mismatched_sizes=True,)\n",
    "        self.train_mean_iou = evaluate.load(\"mean_iou\")\n",
    "        self.val_mean_iou = evaluate.load(\"mean_iou\")\n",
    "        self.test_mean_iou = evaluate.load(\"mean_iou\")\n",
    "        \n",
    "    def forward(self, images, masks):\n",
    "        outputs = self.model(pixel_values=images, labels=masks)\n",
    "        return(outputs)\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        outputs = self(images, masks)\n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        upsampled_logits = nn.functional.interpolate(logits, size=masks.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "        self.train_mean_iou.add_batch(predictions=predicted.detach().cpu().numpy(), references=masks.detach().cpu().numpy())\n",
    "        if batch_nb % self.metrics_interval == 0:\n",
    "            metrics = self.train_mean_iou.compute(num_labels=self.num_classes, ignore_index=255, reduce_labels=False,)\n",
    "            metrics = {'loss': loss, \"mean_iou\": metrics[\"mean_iou\"], \"mean_accuracy\": metrics[\"mean_accuracy\"]}\n",
    "            for k,v in metrics.items():\n",
    "                self.log(k,v)\n",
    "            return(metrics)\n",
    "        else:\n",
    "            return({'loss': loss})\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        outputs = self(images, masks)\n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        upsampled_logits = nn.functional.interpolate(logits, size=masks.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "        self.val_mean_iou.add_batch(predictions=predicted.detach().cpu().numpy(), references=masks.detach().cpu().numpy())\n",
    "        return({'val_loss': loss})\n",
    "    \n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        outputs = self(images, masks)\n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        upsampled_logits = nn.functional.interpolate(logits, size=masks.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "        self.test_mean_iou.add_batch(predictions=predicted.detach().cpu().numpy(), references=masks.detach().cpu().numpy())\n",
    "        return({'test_loss': loss})\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=2e-05, eps=1e-08)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.train_dl\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.val_dl\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self.test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SegformerFineTuner=SegformerFinetuner(id2label=id2label,train_dataloader=train_dataloader,val_dataloader=valid_dataloader,test_dataloader=test_dataloader,metrics_interval=10)\n",
    "SegformerFineTuner.to(device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=pl.Trainer(max_epochs=10,val_check_interval=len(valid_dataloader),accelerator=\"gpu\",devices=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model=SegformerFineTuner,train_dataloaders=train_dataloader,val_dataloaders=valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
